{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce91ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# disable progress bars because it breaks model downloading for some reason\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, einsum\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1006d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_data = pd.read_parquet(\"hf://datasets/tatsu-lab/alpaca/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet\")\n",
    "harmful_data = pd.read_csv('/nfshomes/dkim5124/Documents/catch-and-steer/data/harmful_behaviors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_inst = [[{\"role\": \"user\", \"content\": text}] for text in harmful_data['goal']]\n",
    "harmless_inst = [[{\"role\": \"user\", \"content\": text}] for text in harmless_data['instruction']]\n",
    "\n",
    "hate_data = [[{\"role\": \"user\", \"content\": text}] for text in ['hate','i hate this', 'hating the', 'hater', 'hating', 'hated in']]\n",
    "#hate_data = [[{\"role\": \"user\", \"content\": text}] for text in ['','', '', '', '', '']]\n",
    "love_data = [[{\"role\": \"user\", \"content\": text}] for text in ['love','i love this', 'loving the', 'lover', 'loving', 'loved in']]\n",
    "my_data = [[{\"role\": \"user\", \"content\": text}] for text in ['Tell me a short story.', 'I do not like you.', 'Tell me a joke.', 'Give me an sentence with an alliteration.', \"I hate you.\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions(tokenizer, instructions):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        instructions,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        add_generation_prompt=True,\n",
    "    ).input_ids\n",
    "\n",
    "lens = 200\n",
    "\n",
    "harmful_toks = tokenize_instructions(tokenizer, harmful_inst[:lens]) # [len, seq_len]\n",
    "harmless_toks = tokenize_instructions(tokenizer, harmless_inst[:lens])\n",
    "harmful_toks_test = tokenize_instructions(tokenizer, harmful_inst[lens:lens+10])\n",
    "hate_toks = tokenize_instructions(tokenizer, hate_data)\n",
    "love_toks = tokenize_instructions(tokenizer, love_data)\n",
    "my_toks_test = tokenize_instructions(tokenizer, my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_steering_vecs(model, base_toks, target_toks, batch_size = 16): \n",
    "    '''\n",
    "    We want to find the steering vector from base_toks to target_toks (we do target_toks - base_toks)\n",
    "    Inputs: \n",
    "        :param model: the model to use\n",
    "        :param base_toks: the base tokens [len, seq_len]\n",
    "        :param target_toks: the target tokens [len, seq_len]\n",
    "    Output: \n",
    "        :return steering_vecs: the steering vectors [hidden_size]\n",
    "    '''\n",
    "    device = model.device\n",
    "    num_its = len(range(0, base_toks.shape[0], batch_size))\n",
    "    steering_vecs = {}\n",
    "    for i in tqdm(range(0, base_toks.shape[0], batch_size)): \n",
    "        # pass through the model \n",
    "        base_out = model(\n",
    "            base_toks[i:i+batch_size].to(device), \n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        ).hidden_states # tuple of length num_layers with each element size [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        target_out = model(target_toks[i:i+batch_size].to(device),\n",
    "        output_hidden_states=True,\n",
    "        use_cache=False,\n",
    "        return_dict=True\n",
    "        ).hidden_states\n",
    "        for layer in range(len(base_out)): \n",
    "            # average over the batch_size, take last token \n",
    "            if i == 0: \n",
    "                steering_vecs[layer] = torch.mean(target_out[layer][:,-1,:].detach().cpu() - base_out[layer][:,-1,:].detach().cpu(), dim=0)/num_its # [hidden_size]\n",
    "            else: \n",
    "                steering_vecs[layer] += torch.mean(target_out[layer][:,-1,:].detach().cpu() - base_out[layer][:,-1,:].detach().cpu(), dim=0)/num_its\n",
    "    return steering_vecs\n",
    "\n",
    "def do_steering(model, test_toks, steering_vec, scale = 1, normalise = True, layer = None, proj=True, batch_size=16): \n",
    "    '''\n",
    "    Input: \n",
    "        :param model: the model to use\n",
    "        :param test_toks: the test tokens [len, seq_len]\n",
    "        :param steering_vec: the steering vector [hidden_size]\n",
    "        :param scale: the scale to use\n",
    "        :param layer: the layer to modify; if None: we modify all layers. \n",
    "        :param proj: whether to project the steering vector\n",
    "    Output:\n",
    "        :return output: the steered model output [len, generated_seq_len]\n",
    "    '''\n",
    "    # define a hook to modify the input into the layer\n",
    "    if steering_vec is not None: \n",
    "        def modify_activation():\n",
    "            def hook(model, input): \n",
    "                if normalise:\n",
    "                    sv = steering_vec / steering_vec.norm()\n",
    "                else: \n",
    "                    sv = steering_vec\n",
    "                if proj:\n",
    "                    sv = einsum(input[0], sv.view(-1,1), 'b l h, h s -> b l s') * sv\n",
    "                input[0][:,:,:] = input[0][:,:,:] - scale * sv\n",
    "            return hook\n",
    "        handles = [] \n",
    "        for i in range(len(model.model.layers)):\n",
    "            if layer is None: # append to each layer\n",
    "                handles.append(model.model.layers[i].register_forward_pre_hook(modify_activation()))\n",
    "            elif layer is not None and i == layer:\n",
    "                handles.append(model.model.layers[i].register_forward_pre_hook(modify_activation()))\n",
    "\n",
    "    gen_kwargs = dict(num_beams=4, do_sample=True, max_new_tokens=60, use_cache=False)\n",
    "\n",
    "    # pass through the model\n",
    "    outs_all = []\n",
    "    for i in tqdm(range(0, test_toks.shape[0], batch_size)):\n",
    "        batch = test_toks[i:i+batch_size].to(device)\n",
    "        outs = model.generate(batch,**gen_kwargs) # [num_samples, seq_len]\n",
    "        outs_all.append(outs)\n",
    "    outs_all = torch.cat(outs_all, dim=0)\n",
    "    # remove all hooks\n",
    "    if steering_vec is not None: \n",
    "        for handle in handles: \n",
    "            handle.remove()\n",
    "    return outs_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
