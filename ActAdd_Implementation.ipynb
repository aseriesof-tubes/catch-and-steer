{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce91ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# disable progress bars because it breaks model downloading for some reason\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, einsum\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1006d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_data = pd.read_parquet(\"hf://datasets/tatsu-lab/alpaca/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet\")\n",
    "harmful_data = pd.read_csv('/nfshomes/dkim5124/Documents/catch-and-steer/data/harmful_behaviors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_inst = [[{\"role\": \"user\", \"content\": text}] for text in harmful_data['goal']]\n",
    "harmless_inst = [[{\"role\": \"user\", \"content\": text}] for text in harmless_data['instruction']]\n",
    "\n",
    "hate_data = [[{\"role\": \"user\", \"content\": text}] for text in ['hate','i hate this', 'hating the', 'hater', 'hating', 'hated in']]\n",
    "#hate_data = [[{\"role\": \"user\", \"content\": text}] for text in ['','', '', '', '', '']]\n",
    "love_data = [[{\"role\": \"user\", \"content\": text}] for text in ['love','i love this', 'loving the', 'lover', 'loving', 'loved in']]\n",
    "my_data = [[{\"role\": \"user\", \"content\": text}] for text in ['Tell me a short story.', 'I do not like you.', 'Tell me a joke.', 'Give me an sentence with an alliteration.', \"I hate you.\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62202844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions(tokenizer, instructions):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        instructions,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        add_generation_prompt=True,\n",
    "    ).input_ids\n",
    "\n",
    "lens = 200\n",
    "\n",
    "harmful_toks = tokenize_instructions(tokenizer, harmful_inst[:lens]) # [len, seq_len]\n",
    "harmless_toks = tokenize_instructions(tokenizer, harmless_inst[:lens])\n",
    "harmful_toks_test = tokenize_instructions(tokenizer, harmful_inst[lens:lens+10])\n",
    "hate_toks = tokenize_instructions(tokenizer, hate_data)\n",
    "love_toks = tokenize_instructions(tokenizer, love_data)\n",
    "my_toks_test = tokenize_instructions(tokenizer, my_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
