ITI uses the fact that LLM activations can be clustered to show "factuality"

It then uses classification to se which heads have the firmest grasp on this idea

Then it uses mean mass shifting to steer towards truthfulness


This is copying that idea with 3 changes

First, focusing on harmfulness instead of truthfulness

Second, to show that multisteering won't work if its not conditional

Third, to show conditional steering